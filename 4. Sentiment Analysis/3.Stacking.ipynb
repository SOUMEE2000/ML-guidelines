{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Stacking.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPSFUB3vuVXA2hlWef0gy4+",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SOUMEE2000/Sentiment-Analysis-guidelines-IMDB-Datset-/blob/main/3.Stacking.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIy6-UHkTvOs"
      },
      "source": [
        "# **Welcome to Stacking and Ensemble Models !!!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ryaEEOVtTAgm"
      },
      "source": [
        "**In the previous notebook we had seen how out of all the classifiers available SVM gaves us the best results. However, each of those calls took about 4 hours and that is definitely not practical. Turns out we can do better. We can stack classifiers one on top of another and they take way less time amd in some cases give better accuracy!!** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yjePQC4kNa0c"
      },
      "source": [
        "source=\"https://machinelearningmastery.com/stacking-ensemble-machine-learning-with-python/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LGdLAkDVNwTL",
        "outputId": "a107f31b-81c8-440b-bfa4-540efaf54543"
      },
      "source": [
        "!unzip IMDB.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  IMDB.zip\n",
            "  inflating: IMDB Dataset.csv        \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "i5TMTbTiN3tz",
        "outputId": "19485ce6-fdd2-4854-f725-84324b431139"
      },
      "source": [
        "import pandas as pd\r\n",
        "df= pd.read_csv(\"IMDB Dataset.csv\")\r\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>One of the other reviewers has mentioned that ...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I thought this was a wonderful way to spend ti...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Basically there's a family where a little boy ...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              review sentiment\n",
              "0  One of the other reviewers has mentioned that ...  positive\n",
              "1  A wonderful little production. <br /><br />The...  positive\n",
              "2  I thought this was a wonderful way to spend ti...  positive\n",
              "3  Basically there's a family where a little boy ...  negative\n",
              "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMnW1hKwOIjr"
      },
      "source": [
        "feature=[]\r\n",
        "for i in df[\"sentiment\"]:\r\n",
        "  if i==\"positive\":\r\n",
        "    feature.append(1)\r\n",
        "  elif i==\"negative\":\r\n",
        "    feature.append(0)\r\n",
        "df[\"feature\"]=feature"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GV8kZNXAPNFw",
        "outputId": "999806ac-f620-44b6-ceb6-278713d64009"
      },
      "source": [
        "df['review_processed'] = df['review'].str.replace(\"[^a-zA-Z#]\", \" \") \r\n",
        "df['review_processed']=[review.lower() for review in df['review_processed']]\r\n",
        "\r\n",
        "# Removing Stopwords Begin\r\n",
        "import nltk\r\n",
        "nltk.download('punkt')\r\n",
        "nltk.download('stopwords')\r\n",
        "from nltk.corpus import stopwords\r\n",
        "from nltk import word_tokenize\r\n",
        "stop_words = stopwords.words('english')\r\n",
        "\r\n",
        "# Making custom list of words to be removed \r\n",
        "add_words = ['movie','film','one','make','even','the']\r\n",
        "stop_words.extend(add_words)\r\n",
        "\r\n",
        "# Function to remove stop words \r\n",
        "def remove_stopwords(rev):\r\n",
        "    review_tokenized = word_tokenize(rev)\r\n",
        "    rev_new = \" \".join([i for i in review_tokenized  if i not in stop_words])\r\n",
        "    return rev_new\r\n",
        "\r\n",
        "# Removing stopwords\r\n",
        "df['review_processed'] = [remove_stopwords(r) for r in df['review_processed']]\r\n",
        "\r\n",
        "# Replacing short words\r\n",
        "df['review_processed'] = df['review_processed'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>2]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NEwLTnFvQFDa"
      },
      "source": [
        "# Importing module\r\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
        "\r\n",
        "# Creating sparse matrix of top 2500 tokens\r\n",
        "cv = TfidfVectorizer(max_features = 2500)\r\n",
        "X = cv.fit_transform(df.review_processed).toarray()\r\n",
        "y = df.feature.values\r\n",
        "\r\n",
        "# Splitting the dataset into train and test\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rEp1eNPwS9Yi"
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\r\n",
        "scaling = MinMaxScaler(feature_range=(0,1)).fit(X_train)\r\n",
        "X_train = scaling.transform(X_train)\r\n",
        "X_test = scaling.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "wlOqTHNJ-T_I"
      },
      "source": [
        "from sklearn.naive_bayes import GaussianNB, MultinomialNB, ComplementNB\r\n",
        "from sklearn.ensemble import RandomForestClassifier\r\n",
        "from sklearn.ensemble import StackingClassifier\r\n",
        "from sklearn.svm import SVC\r\n",
        "from sklearn.linear_model import LogisticRegression\r\n",
        "models= []\r\n",
        "models.append((\"GNB\", GaussianNB())) \r\n",
        "models.append((\"CNB\", ComplementNB()))\r\n",
        "models.append((\"MNB\", MultinomialNB()))\r\n",
        "models.append((\"RF\", RandomForestClassifier(max_depth=500, n_estimators=1000, max_features=5, min_samples_split=5)))\r\n",
        "models.append((\"LR\", LogisticRegression()))\r\n",
        "models.append((\"SVC\", SVC(kernel=\"linear\")))\r\n",
        "model= StackingClassifier(estimators=models)\r\n",
        "model.fit(X_train, y_train)   \r\n",
        "y_pred= model.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "yyJDaj0fQYeC",
        "outputId": "757e679a-2fc8-48ca-c793-97fbdf416ce1"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score\r\n",
        "accuracy = accuracy_score(y_test, y_pred)\r\n",
        "cm = confusion_matrix(y_test, y_pred)\r\n",
        "print(cm)\r\n",
        "print(\"The model accuracy is\", accuracy )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[4430  605]\n",
            " [ 567 4398]]\n",
            "The model accuracy is 0.8828\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zE9-QY_o8JKf"
      },
      "source": [
        "# **Some Results**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixkxpN1ST_Zj"
      },
      "source": [
        "* StackingClassifier(estimators=(GNB,CNB,MNB)), 84.94%\r\n",
        "* StackingClassifier(estimators=(GNB,CNB,MNB),final_estimator=RandomForestClassifier(max_depth=500)), 81.23%\r\n",
        "* StackingClassifier(estimators=(GNB,CNB,MNB,RF(max_depth=500)), 86.64%\r\n",
        "* StackingClassifier(estimators=(GNB,CNB,MNB,RF(max_depth=500)),final_estimator=LogisticRegression()), 86.72%\r\n",
        "* StackingClassifier(estimators=(GNB,CNB,MNB,RF(max_depth=500), LR)), 88.25%\r\n",
        "* StackingClassifier(estimators=(GNB,CNB,MNB,RF(max_depth=500), LSVC), 88.34%"
      ]
    }
  ]
}
