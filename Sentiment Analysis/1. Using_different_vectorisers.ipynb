{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Implementing different vectorisers.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SOUMEE2000/Sentiment-Analysis-guidelines-IMDB-Datset-/blob/main/1.%20Using_different_vectorisers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SzRskbZSOo28"
      },
      "source": [
        "**SKlearn has a variety of different hash vectorisers and it can be interesting to know how even things such vectoriser, stemming and lemmastisation can affect the the accuracy even though they are not directly related to the model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jY26R7H-N4dZ",
        "outputId": "0cff656f-21fc-4834-90d2-4a08c9cdb6cb"
      },
      "source": [
        "!unzip IMDB.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  IMDB.zip\n",
            "  inflating: IMDB Dataset.csv        \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wyqN0_3VLOMr"
      },
      "source": [
        "import pandas as pd\n",
        "df= pd.read_csv('IMDB Dataset.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "BxHH0uJVLONG",
        "outputId": "01de8ab4-ae65-4502-8ef0-23a8f06d0db5"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>One of the other reviewers has mentioned that ...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I thought this was a wonderful way to spend ti...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Basically there's a family where a little boy ...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              review sentiment\n",
              "0  One of the other reviewers has mentioned that ...  positive\n",
              "1  A wonderful little production. <br /><br />The...  positive\n",
              "2  I thought this was a wonderful way to spend ti...  positive\n",
              "3  Basically there's a family where a little boy ...  negative\n",
              "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOAba7nBLONQ"
      },
      "source": [
        "# make entire text lowercase\n",
        "df['review_processed'] = [review.lower() for review in df['review']]\n",
        "\n",
        "# Replacing short words\n",
        "df['review_processed'] = df['review_processed'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>2]))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RZT8j7UkLONL",
        "outputId": "3c491940-606a-490b-c01f-090c35eb2073"
      },
      "source": [
        "# Removing Stopwords Begin\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import word_tokenize\n",
        "stop_words = stopwords.words('english')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P234-mNELONM"
      },
      "source": [
        "# Making custom list of words to be removed \n",
        "add_words = ['movie','film','one','make','even','the']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tOx-pNILLONN"
      },
      "source": [
        "stop_words.extend(add_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2sM4xfdTLONP"
      },
      "source": [
        "# Function to remove stop words \n",
        "def remove_stopwords(rev):\n",
        "    review_tokenized = word_tokenize(rev)\n",
        "    rev_new = \" \".join([i for i in review_tokenized  if i not in stop_words])\n",
        "    return rev_new\n",
        "\n",
        "# Removing stopwords\n",
        "df['review_processed'] = [remove_stopwords(r) for r in df['review_processed']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AWgHXum9Q7q2"
      },
      "source": [
        "feature=[]\r\n",
        "for i in df[\"sentiment\"]:\r\n",
        "    if i== \"positive\":\r\n",
        "        feature.append(1)\r\n",
        "    elif i ==\"negative\":          # if \"negative\" was not specified, length of df wouldn't match with len(features)\r\n",
        "        feature.append(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9OWu8SIARHgI"
      },
      "source": [
        "df[\"feature\"]= feature"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "id": "vZ0Ro0beRMsE",
        "outputId": "fcfe052f-15e1-4939-e5bd-64014e2e5125"
      },
      "source": [
        "# Begin Lemmatization \r\n",
        "\"\"\"from nltk.stem import WordNetLemmatizer\r\n",
        "from nltk.corpus import wordnet\r\n",
        "nltk.download('wordnet') # Run atleast once \r\n",
        "nltk.download('averaged_perceptron_tagger')\r\n",
        "# function to convert nltk tag to wordnet tag\r\n",
        "lemmatizer = WordNetLemmatizer()\r\n",
        "\r\n",
        "# Finds the part of speech tag\r\n",
        "def nltk_tag_to_wordnet_tag(nltk_tag):\r\n",
        "    if nltk_tag.startswith('J'):\r\n",
        "        return wordnet.ADJ\r\n",
        "    elif nltk_tag.startswith('V'):\r\n",
        "        return wordnet.VERB\r\n",
        "    elif nltk_tag.startswith('N'):\r\n",
        "        return wordnet.NOUN\r\n",
        "    elif nltk_tag.startswith('R'):\r\n",
        "        return wordnet.ADV\r\n",
        "    else:          \r\n",
        "        return None\r\n",
        "\r\n",
        "# lemmatize sentence using pos tag\r\n",
        "def lemmatize_sentence(sentence):\r\n",
        "    #tokenize the sentence and find the POS tag for each token\r\n",
        "    nltk_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))  \r\n",
        "    #tuple of (token, wordnet_tag)\r\n",
        "    wordnet_tagged = map(lambda x: (x[0], nltk_tag_to_wordnet_tag(x[1])), nltk_tagged)\r\n",
        "    lemmatized_sentence = []\r\n",
        "    for word, tag in wordnet_tagged:\r\n",
        "        if tag is None:\r\n",
        "            #if there is no available tag, append the token as is\r\n",
        "            lemmatized_sentence.append(word)\r\n",
        "        else:        \r\n",
        "            #else use the tag to lemmatize the token\r\n",
        "            lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\r\n",
        "    return \" \".join(lemmatized_sentence)\r\n",
        "\r\n",
        "\r\n",
        "df['review_processed'] = df['review_processed'].apply(lambda x: lemmatize_sentence(x))\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'from nltk.stem import WordNetLemmatizer\\nfrom nltk.corpus import wordnet\\nnltk.download(\\'wordnet\\') # Run atleast once \\nnltk.download(\\'averaged_perceptron_tagger\\')\\n# function to convert nltk tag to wordnet tag\\nlemmatizer = WordNetLemmatizer()\\n\\n# Finds the part of speech tag\\ndef nltk_tag_to_wordnet_tag(nltk_tag):\\n    if nltk_tag.startswith(\\'J\\'):\\n        return wordnet.ADJ\\n    elif nltk_tag.startswith(\\'V\\'):\\n        return wordnet.VERB\\n    elif nltk_tag.startswith(\\'N\\'):\\n        return wordnet.NOUN\\n    elif nltk_tag.startswith(\\'R\\'):\\n        return wordnet.ADV\\n    else:          \\n        return None\\n\\n# lemmatize sentence using pos tag\\ndef lemmatize_sentence(sentence):\\n    #tokenize the sentence and find the POS tag for each token\\n    nltk_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))  \\n    #tuple of (token, wordnet_tag)\\n    wordnet_tagged = map(lambda x: (x[0], nltk_tag_to_wordnet_tag(x[1])), nltk_tagged)\\n    lemmatized_sentence = []\\n    for word, tag in wordnet_tagged:\\n        if tag is None:\\n            #if there is no available tag, append the token as is\\n            lemmatized_sentence.append(word)\\n        else:        \\n            #else use the tag to lemmatize the token\\n            lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\\n    return \" \".join(lemmatized_sentence)\\n\\n\\ndf[\\'review_processed\\'] = df[\\'review_processed\\'].apply(lambda x: lemmatize_sentence(x))'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "rq0ZKbZER8bk",
        "outputId": "7828f96e-7acc-4dc1-a013-51626ad5bb4e"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>review_processed</th>\n",
              "      <th>feature</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>One of the other reviewers has mentioned that ...</td>\n",
              "      <td>positive</td>\n",
              "      <td>reviewers mentioned watching episode 'll hooke...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
              "      <td>positive</td>\n",
              "      <td>wonderful little production . &lt; br / &gt; &lt; br / ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I thought this was a wonderful way to spend ti...</td>\n",
              "      <td>positive</td>\n",
              "      <td>thought wonderful way spend time hot summer we...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Basically there's a family where a little boy ...</td>\n",
              "      <td>negative</td>\n",
              "      <td>basically 's family little boy ( jake ) thinks...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
              "      <td>positive</td>\n",
              "      <td>petter mattei 's `` love time money '' visuall...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              review  ... feature\n",
              "0  One of the other reviewers has mentioned that ...  ...       1\n",
              "1  A wonderful little production. <br /><br />The...  ...       1\n",
              "2  I thought this was a wonderful way to spend ti...  ...       1\n",
              "3  Basically there's a family where a little boy ...  ...       0\n",
              "4  Petter Mattei's \"Love in the Time of Money\" is...  ...       1\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AuwDyZMuTqZD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b863bec-0145-44cd-bc65-3d0ed45edac4"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
        "vectorizer= TfidfVectorizer(max_features=2500)\r\n",
        "vectorizer.fit(df.review_processed)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
              "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
              "                input='content', lowercase=True, max_df=1.0, max_features=2500,\n",
              "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
              "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
              "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                tokenizer=None, use_idf=True, vocabulary=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxeN0JXQUju9"
      },
      "source": [
        "X = vectorizer.transform(df.review_processed).toarray()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m5p4OFj1UoOm"
      },
      "source": [
        "y= df.feature.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ayoHA6AMZqF0"
      },
      "source": [
        "#from sklearn.preprocessing import MinMaxScaler\r\n",
        "#scaling = MinMaxScaler(feature_range=(0,1)).fit(X)\r\n",
        "#X = scaling.transform(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7i_8t2DNaZCF",
        "outputId": "a1333569-3b5d-47ea-c9f7-b20d42b67c90"
      },
      "source": [
        "X"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YNidAmhfVbcQ"
      },
      "source": [
        "# Splitting the dataset into train and test\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UBCG3H9VpX1"
      },
      "source": [
        "import numpy as np\r\n",
        "from sklearn.naive_bayes import MultinomialNB\r\n",
        "classifier= MultinomialNB()\r\n",
        "classifier.fit(X_train, y_train)\r\n",
        "## Testing the model on test set\r\n",
        "y_pred = classifier.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QF1J2CBnWf9L",
        "outputId": "aac7f407-01b6-4f9b-c6c5-73b2d5ed7ac5"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score\r\n",
        "accuracy = accuracy_score(y_test, y_pred)\r\n",
        "cm = confusion_matrix(y_test, y_pred)\r\n",
        "print(cm)\r\n",
        "print(\"The model accuracy is\", accuracy )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[4230  805]\n",
            " [ 698 4267]]\n",
            "The model accuracy is 0.8497\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIlKCW7082sL"
      },
      "source": [
        "# **Some Results**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wavi7xggNEGk"
      },
      "source": [
        " **Hash Vectoriser**\r\n",
        " \r\n",
        " * naive_bayes(GNB) = 70.93%\r\n",
        " * naive_bayes(MNB),(CNB) somehow negative values are being passed\r\n",
        " * To get around that used the MinMax scaler CNB= 77.5%, MNB=76.8%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOYUNHpqN0LJ"
      },
      "source": [
        "**Tfidf vectorizer**\r\n",
        "* GNB= 81.01%   81.75%(after properly removing stopwords)\r\n",
        "* MNB= 84.69%   84.97%(,,)\r\n",
        "* CNB= 84.7%    85%(,,)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNWjTBpHOJIz"
      },
      "source": [
        "**Without stopwords lemmatisation takes longer, CNB= 84.22%, GNB= 80.52%, MNB= 84.27%(surprisingly)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8S-9RN99OTP5"
      },
      "source": [
        "**Without lemmatisation, MNB= 84.42%, CNB= 84.46%, GNB=81.92%**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gpmJyx4OdeT"
      },
      "source": [
        "**With optimisation GNB= 81.92%**"
      ]
    }
  ]
}
